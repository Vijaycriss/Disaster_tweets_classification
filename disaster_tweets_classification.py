# -*- coding: utf-8 -*-
"""Disaster_tweets_classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RAYYERcIl65MpXFaz1pEn22OEgzXhxGJ
"""

import pandas as pd
import numpy as np

df = pd.read_csv('/content/disaster_tweets_data(DS).csv')

df.head()

df.shape

df.isnull().sum()

df['target'].value_counts()

df.info()

"""##Tokenizing"""

'''import nltk
nltk.download('punkt')
from nltk.tokenize import word_tokenize

df['tweets'] = df['tweets'].apply(lambda x : word_tokenize(x))
print(df.head())'''

df.tail()

"""#Convert words to lower case"""

df["tweets"] = df["tweets"].apply(lambda x: x.lower())
df.head()

"""##Removing Punctuations"""

import re

# removing punctuation
df['tweets'] = df['tweets'].str.replace(r'[^\w\s]+',' ').str.strip()

#removing URls
df['tweets'] = df['tweets'].str.replace(r'/s*https?:///S+(/s+|$)', '').str.strip()

df['tweets'] = df['tweets'].str.replace('/d+', '')

df.head()

"""##Removing Stop words"""

import nltk
from nltk.corpus import stopwords

# Removing the Stop Words From the Text Column

nltk.download('stopwords')

stop = set(stopwords.words("english"))


def remove_stopwords(text):
  filtered_words =  [word.lower() for word in text.split() if word.lower() not in stop]
  return " ".join(filtered_words)


df["tweets"] = df["tweets"].map(remove_stopwords)

df.head()

"""##Stemming or lemmatizing the words"""

from nltk.stem import PorterStemmer
from nltk.stem import WordNetLemmatizer
nltk.download('wordnet')

# Apply Lemamtization
lemmatizer = WordNetLemmatizer()
def lemmatize_words(text):
    return " ".join([lemmatizer.lemmatize(word) for word in text.split()])

df["tweets"] = df["tweets"].apply(lambda text: lemmatize_words(text))
df.head()

df.head()

"""##Train and Test Split"""

x = df['tweets']
y = df['target']

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(x, y, stratify = y, test_size = 0.2, random_state= 0)

"""##Count Vectorizer"""

from sklearn.feature_extraction.text import CountVectorizer

count_vectorizer = CountVectorizer(stop_words='english')

count_train = count_vectorizer.fit_transform(X_train)

count_test = count_vectorizer.transform(X_test)

"""##Multinomial Naïve Bayes Classification"""

from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score
from sklearn import metrics

mnb = MultinomialNB()
mnb.fit(count_train, y_train)
pred = mnb.predict(count_test)
score = metrics.accuracy_score(y_test, pred)
print("accuracy:   %0.3f" % score)

"""##Logistic Regression"""

from sklearn.linear_model import LogisticRegression

lg = LogisticRegression()
lg.fit(count_train, y_train)
pred_lg = lg.predict(count_test)
score = metrics.accuracy_score(y_test, pred_lg)
print("accuracy:  %0.3f" % score)

"""##KNN Classification"""

from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(n_neighbors=1)
knn.fit(count_train, y_train)
pred_knn = knn.predict(count_test)
score = metrics.accuracy_score(y_test, pred_knn)
print("accuracy:  %0.3f" % score)

pred = mnb.predict(count_test)
pred_lg = lg.predict(count_test)
pred_knn = knn.predict(count_test)
print(pred)
print(pred_lg)
print(pred_knn)

"""##Confusion matrix and classification report"""

from sklearn.metrics import confusion_matrix, classification_report

confusion_NB = confusion_matrix(y_test, pred)
confusion_LG = confusion_matrix(y_test, pred_lg)
confusionKNN = confusion_matrix(y_test, pred_knn)

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(10,6))
sns.heatmap(confusion_NB, annot=True, fmt="d", cmap="Blues",xticklabels=['Positive','Negative'], yticklabels=['Positive','Negative'])
plt.ylabel('Prediction',fontsize=13)
plt.xlabel('Actual',fontsize=13)
plt.title('Confusion Matrix for NB',fontsize=17)
plt.show()

report_NB = classification_report(y_test, pred)
print(f'Classification Report:\n{report_NB}')

plt.figure(figsize=(10,6))
sns.heatmap(confusion_LG, annot=True, fmt="d",xticklabels=['Positive','Negative'], yticklabels=['Positive','Negative'])
plt.ylabel('Prediction',fontsize=13)
plt.xlabel('Actual',fontsize=13)
plt.title('Confusion Matrix for LG',fontsize=17)
plt.show()

report_LG = classification_report(y_test, pred_lg)
print(f'Classification Report:\n{report_LG}')

plt.figure(figsize=(10,6))
sns.heatmap(confusionKNN, annot=True, fmt="d",xticklabels=['Positive','Negative'], yticklabels=['Positive','Negative'])
plt.ylabel('Prediction',fontsize=13)
plt.xlabel('Actual',fontsize=13)
plt.title('Confusion Matrix for KNN',fontsize=17)
plt.show()

report_KNN = classification_report(y_test, pred_knn)
print(f'Classification Report:\n{report_KNN}')

"""##model with the best accuracy."""

accuracy_nb = accuracy_score(y_test, pred)
accuracy_lg = accuracy_score(y_test, pred_lg)
accuracy_knn = accuracy_score(y_test, pred_knn)

accuracy_scores = {
    'Multinomial Naïve Bayes': accuracy_nb,
    'Logistic Regression': accuracy_lg,
    'K-Nearest Neighbors': accuracy_knn
}
best_model = max(accuracy_scores, key=accuracy_scores.get)
best_accuracy = accuracy_scores[best_model]

print("Model with the best accuracy:", best_model)
print("Best accuracy:", best_accuracy * 100)